{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec are a family of algorithms that can turn words in vectors for us. These vectors capture the semantic mean of the words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "When we want to feed text to a deep learning model one approach is to onehot encode the words however another approach is the use of word embedding, also called  word vectors. In contrast to onehot encoded vectors which are sparse (mostly made of 0s), word vectors contain few zeros and mostly numbers. In addtion they are low dimensionality often bellow 1000D, which is much smaller than a onehot encoded vector which can be 20,000D or greater (depending on the size of the volcabulary). \n",
    "dimensionality\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png\" algin=\"center\" width=\"600\" >\n",
    "\n",
    "\n",
    "Word embeddings are learned from the data and as such capture the semantic meanings of words by mapping them to some geometric space. To elborate two similar words such as 'exact' and 'accurate' would be closer together in the word embedding space, than two underelated words like fish and London.  Because of how word vector are created it's often better to measure distance using cosine simliarity rather than l2 distance. Since the words are vectors of numbers we can do simple math with them, for example Google researchers (Mikolov 2013)  found if we take the word vectors for king,man and woman we find that: \n",
    "\n",
    "$$ king - man + woman = queen $$.\n",
    "\n",
    "Another good one.\n",
    "\n",
    "$$ president - power  = prime minister $$\n",
    "\n",
    "Word embeddings give us a easy ways to compare two words based on their meaning, which until a few years ago was much harder. The word vectors even managed to capture the realasionship of capitals\n",
    "\n",
    "<img src=\"https://www.springboard.com/blog/wp-content/uploads/2017/08/country-Copy.png\" width=\"600\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have two option when we want to create word embeddings:\n",
    "\n",
    "* Learn the word embeddings overselves, by traning a network on the text relevant to the specific class at hand. \n",
    "* Use a pretrain word embeddding.\n",
    "\n",
    "Well cover how to do both later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram\n",
    "\n",
    "\n",
    "The idea of the skip gram model is to predict  the other context words given a target word. We have a window size  $m$ which is how far to the left and right we'll consider when trying to predict the other context words. Lets say that our window size is 2 then the train samples would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/1*yiH5sZI-IBxDSQMKhvbcHw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can't feed the words into the model directly we'll onehot encode them first.  \n",
    "\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png)\n",
    "\n",
    "We design a loss function that when optimized allows us to maximize the probality of the context words occuring. The intresting part is that after training we then use the models weights (from the embedding matirx) to get our word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip gram isn't the only word2vec model some other word2vec models include cbow and fast text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usecases\n",
    "\n",
    "Word2Vec models have many use cases some examples are:\n",
    "\n",
    "\n",
    "* Web Search\n",
    "    * If a user searches for *dell notebook battery size*, we'd also like to match documents that contain *dell laptop battery capacity*\n",
    "* Sentiment Analysis\n",
    "* Document Classification\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [Learning Word Embeddings](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)\n",
    "* [How does word2vec work](https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
