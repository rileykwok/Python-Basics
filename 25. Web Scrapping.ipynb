{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "\n",
    "Web scrapping is the process of extracting usefull information from web pages, before we can get into web scrapping we should have a basic understanding of the technologies used to create webpages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web technologies\n",
    "\n",
    "There's a tiro of technologies commonly used  in the  creation of webpages:\n",
    "\n",
    "* HTML - used to create the content of the page\n",
    "* CSS - used to style the content. \n",
    "* Javascript - used to add interactivity to pages, add logic and fetch data.\n",
    "\n",
    "\n",
    "The browser rendering engine takes the html and css and renders it into a webpage.  Afterwards any javascript is then excuted, this may change the layout of the page, or fetch some additional data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML \n",
    "HTML (Hyper text markup langauge) consists of html elements or tags which are names surrounded by angle brackets (<>). HTML tags usually come in pairs, for example:\n",
    "\n",
    "```\n",
    "<tagname>content goes here...</tagname>\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Bellow we have an example of what HTML looks like. We can use the ipython html magic (`%%html`) to excute the html in the notebook so we can see how it renders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <h1>This is a heading</h1>\n",
       "    <p>Normal text usually going in paragraph tags</p>\n",
       "    <ul>\n",
       "        <li>Item 1</li>\n",
       "        <li><strong>We can bold text by putting it inside strong tags</strong></li>\n",
       "        <li> <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element\">Link to MDN Docs</a> </li>\n",
       "    </ul>\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<body>\n",
    "    <h1>This is a heading</h1>\n",
    "    <p>Normal text usually going in paragraph tags</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li><strong>We can bold text by putting it inside strong tags</strong></li>\n",
    "        <li> <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element\">Link to MDN Docs</a> </li>\n",
    "    </ul>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML elements can also have attributes, these attributes provide a way to add additional information to the element. For example in the `<a>` tag there is a href attribute which contains a link to another site.\n",
    "\n",
    "```html\n",
    "<a href=\"https://www.javascript.com/\" >JavaScript</a>\n",
    "```\n",
    "\n",
    "Another common tag is the ` <div>` tag. Web developers use div tags to divide up pages and apply styles easily to lots of elements. The div tags often contain a class attribute, which can be targeted with css. \n",
    "\n",
    "\n",
    "```html\n",
    "  <div class=\"red\">\n",
    "        <p>Some text that I want read</p>\n",
    "  </div>\n",
    "```\n",
    "\n",
    "A`<style>` tag can be used to contain CSS which allows us to apply different styling to the html. For illustrative purposes I put the CSS inside the style tag but usally it is within a seperate file called a stylesheet and linked to the html document using a `<link>` tag. Class attribtutes can be reused to apply the same styles to many elements. This is in contrast to id attributes which should be used on a single element. Notice that in CSS classes start with a dot e.g `.classname` , whereas ID's start with a hashtag e.g `#myid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<body>\n",
       "    <style>\n",
       "        .red {\n",
       "            color:red\n",
       "        }\n",
       "        \n",
       "        .hidden {\n",
       "            display:none\n",
       "        }\n",
       "        \n",
       "        #an_id {\n",
       "            color: blue\n",
       "        }\n",
       "    \n",
       "    </style>\n",
       "    <h2 class=\"hidden\">This text will be hidden. Try and remove the hidden class to see what happens.</h2>\n",
       "    <div class=\"red\">\n",
       "        <p> This text will be red because it is surrounded by a div that has a class of red. </p>\n",
       "        <p id=\"an_id\">The styling from IDs has higher proirty over classes, hence me being blue. Ids should be unique</p>\n",
       "    </div>\n",
       "    <img src=\"http://www.catster.com/wp-content/uploads/2017/08/A-fluffy-cat-looking-funny-surprised-or-concerned.jpg\" alt=\"\">\n",
       "</body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<body>\n",
    "    <style>\n",
    "        .red {\n",
    "            color:red\n",
    "        }\n",
    "        \n",
    "        .hidden {\n",
    "            display:none\n",
    "        }\n",
    "        \n",
    "        #an_id {\n",
    "            color: blue\n",
    "        }\n",
    "    \n",
    "    </style>\n",
    "    <h2 class=\"hidden\">This text will be hidden. Try and remove the hidden class to see what happens.</h2>\n",
    "    <div class=\"red\">\n",
    "        <p> This text will be red because it is surrounded by a div that has a class of red. </p>\n",
    "        <p id=\"an_id\">The styling from IDs has higher proirty over classes, hence me being blue. Ids should be unique</p>\n",
    "    </div>\n",
    "    <img src=\"http://www.catster.com/wp-content/uploads/2017/08/A-fluffy-cat-looking-funny-surprised-or-concerned.jpg\" alt=\"\">\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Html is infact a type of tree structure so when describing html you may hear the use words such as parent, child, sibling and ancestor node. A child node is anything that is contained within an element, for example the p tags within the div are children of the div. Siblings are elments that are next to each other in the tree for example the two p tags above are siblings. A parents node is the node directly above an element.  For more on the HTML Tree see [here](https://javascript.info/dom-nodes).  There are many html elements, a good resource to learn more about them and web tech in general is MDN (mozilla development network) [documentation](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is one final example of a table in html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "  <tr>   \n",
       "    <th>Firstname</th>\n",
       "    <th>Lastname</th> \n",
       "    <th>Age</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Jill</td>\n",
       "    <td>Smith</td> \n",
       "    <td>50</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Eve</td>\n",
       "    <td>Jackson</td> \n",
       "    <td>94</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table style=\"width:100%\">\n",
    "  <tr>   \n",
    "    <th>Firstname</th>\n",
    "    <th>Lastname</th> \n",
    "    <th>Age</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Jill</td>\n",
    "    <td>Smith</td> \n",
    "    <td>50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Eve</td>\n",
    "    <td>Jackson</td> \n",
    "    <td>94</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome Development Tools\n",
    "\n",
    "Before we can scrape a webpage we need to understand it's structure, for this we'll use the chrome developer tools to inspect it. Most of the browsers have development tools but chrome's are among the best. The development tools allow us to inspect a html page easily so that we can find the html elements which we would like to extract information from. They also allow us to look at network requests to see if the page fetching or sending data via AJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP\n",
    "\n",
    "HTTP (Hyper text transfer protocol) is a request-response protocol that we use to send and recieve information on the internet. The protocol supports many [methods](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods) for example when you go on a webpage your browser makes a GET request, and the sever will repsond with the HTML page, some metadata (a header ) and some kind of status code. Likewise when your on a website and you fill out a form, when you press the submit button you'll send the forms information to the server using a POST request, and server will respond to comform it recieved the POST request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/42_(number)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url) #make get request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has many methods. Bellow we look at the status code of the response to check if it was successfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code #200 means succesfull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headers allow us to contain additional information we sending and responding to HTTP requests, such as if the request is coming from a laptop or a mobile or which browser was used to make the request. In the response headers bellow we can see some additional information like character encoding ('utf-8') and that the content has been gzipped. The browser will make use of this information to render the page correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X-UA-Compatible': 'IE=Edge', 'X-Cache-Status': 'hit-front', 'P3P': 'CP=\"This is not a P3P policy! See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"', 'Vary': 'Accept-Encoding,Cookie,Authorization', 'Backend-Timing': 'D=121814 t=1524331584206864', 'Accept-Ranges': 'bytes', 'Link': '</static/images/project-logos/enwiki.png>;rel=preload;as=image;media=not all and (min-resolution: 1.5dppx),</static/images/project-logos/enwiki-1.5x.png>;rel=preload;as=image;media=(min-resolution: 1.5dppx) and (max-resolution: 1.999999dppx),</static/images/project-logos/enwiki-2x.png>;rel=preload;as=image;media=(min-resolution: 2dppx)', 'Content-Length': '47707', 'Set-Cookie': 'WMF-Last-Access=24-Apr-2018;Path=/;HttpOnly;secure;Expires=Sat, 26 May 2018 00:00:00 GMT, WMF-Last-Access-Global=24-Apr-2018;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Sat, 26 May 2018 00:00:00 GMT, GeoIP=US:TX:San_Antonio:29.42:-98.49:v4; Path=/; secure; Domain=.wikipedia.org', 'Cache-Control': 'private, s-maxage=0, max-age=0, must-revalidate', 'Last-Modified': 'Sat, 21 Apr 2018 17:21:23 GMT', 'X-Varnish': '613611169 620141194, 1024510988 825052123, 261401901 991297513', 'X-Cache': 'cp1066 hit/2, cp2010 hit/7, cp2004 hit/9', 'Content-Encoding': 'gzip', 'Date': 'Tue, 24 Apr 2018 03:01:29 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Content-language': 'en', 'X-Client-IP': '13.84.167.194', 'X-Content-Type-Options': 'nosniff', 'Connection': 'keep-alive', 'X-Powered-By': 'HHVM/3.18.6-dev', 'Via': '1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1)', 'Server': 'mw1329.eqiad.wmnet', 'X-Analytics': 'ns=0;page_id=191178;https=1;nocookies=1', 'Strict-Transport-Security': 'max-age=106384710; includeSubDomains; preload', 'Age': '121308'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're really intrested in is the responses text in other words the HTML document itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars:  250772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>42 (number) - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"42_(number)\",\"wgTitle\":\"42 (number)\",\"wgCurRevisionId\":837567490,\"wgRevisionId\":837567490,\"wgArticleId\":191178,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1: Julian–Gregorian uncertainty\",\"All articles with dead external links\",\"Articles with dead external links from December 2017\",\"Articles with permanently dead external links\",\"Articles needing additional references from February 2011\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = response.text #the html\n",
    "print(\"Number of chars: \",len(html))\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it would be very tricky to extract information out of this  large html document without some help, this is where beautiful soup comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful soup\n",
    "\n",
    "Beauitiful soup will help us parse the html and return an easy to use object, using this object we can get anything we want from the page. First however we need to spot what information we want and what kind of HTML element is it contained in. Does that element always have a certain attribute, or some other unique way to identify it? The easiest way to do this is using the chrome developer tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag,Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,\"lxml\") #lxml is a fast html parser written in C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "Using `soup.find_all` we can extract all of the `<a>` tags from a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a id=\"top\"></a>,\n",
       " <a href=\"#mw-head\">navigation</a>,\n",
       " <a href=\"#p-search\">search</a>,\n",
       " <a class=\"image\" href=\"/wiki/File:Question_book-new.svg\"><img alt=\"\" data-file-height=\"399\" data-file-width=\"512\" height=\"39\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" width=\"50\"/></a>,\n",
       " <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">verification</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags = soup.find_all('a') #get all a tags\n",
    "a_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want `<a>` tags with a href attibute, we have to pass `href=True` to find_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all('a',href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#p-search',\n",
       " '/wiki/File:Question_book-new.svg',\n",
       " '/wiki/Wikipedia:Verifiability',\n",
       " '//en.wikipedia.org/w/index.php?title=42_(number)&action=edit']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrefs = [ a.get('href') for a in a_tags]\n",
    "# hrefs = [ a.attrs['href'] for a in a_tags]\n",
    "hrefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't specify the `<a>` tags `find_all` will return all elements with a `href` attibute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = soup.find_all(href=True) #will also get style sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use [css selectors](https://www.w3schools.com/cssref/css_selectors.asp). We'll cover more on css selectors later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Greek_numerals\" title=\"Greek numerals\">Greek numeral</a>,\n",
       " <a href=\"/wiki/Roman_numerals\" title=\"Roman numerals\">Roman numeral</a>,\n",
       " <a href=\"/wiki/Arabic_numerals\" title=\"Arabic numerals\">Arabic</a>,\n",
       " <a href=\"/wiki/Chinese_numerals\" title=\"Chinese numerals\">Chinese</a>,\n",
       " <a href=\"/wiki/Chuvash_numerals\" title=\"Chuvash numerals\">Chuvash</a>,\n",
       " <a href=\"/wiki/Hebrew_numerals\" title=\"Hebrew numerals\">Hebrew</a>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('a[href*=numerals]') # select all a tags where the href contains the substring numerals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text\n",
    "\n",
    "Often we'll want to extract text from a webpage we can use `get_text` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n42 (number) - Wikipedia\\ndocument.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );\\n(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"42_(number)\",\"wgTitle\":\"42 (number)\",\"wgCurRevisionId\":828403215,\"wgRevisionId\":828403215,\"wgArticleId\":191178,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1: Julian–Gregorian uncertainty\",\"All articles with dead external links\",\"Articles with dead external links from December 2017\",\"Articles with permanently dead external links\",\"Articles needing additional references from February 2011\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from February 2011\",\"Articles containing Afrikaans-language text\",\"Articles containing Albanian-language '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = soup.get_text()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also gets lots of texy that we don't care about. Well have to be more specific, likes only get the text from the p tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><b>42</b> (<b>forty-two</b>) is the <a href=\"/wiki/Natural_number\" title=\"Natural number\">natural number</a> that succeeds <a href=\"/wiki/41_(number)\" title=\"41 (number)\">41</a> and precedes <a href=\"/wiki/43_(number)\" title=\"43 (number)\">43</a>.</p>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tags =  soup.find_all('p')\n",
    "p_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42 (forty-two) is the natural number that succeeds 41 and precedes 43.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tags[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "For extracting tables the pandas package has a really usefull function `read_html` , this won't work on all html tables, but can be good for some. The table might not always be formated in the correct way, but this is often easy to fix in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"http://www.nanotech-now.com/metric-prefix-table.htm\") #download all tables on page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefix</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Multiplier</th>\n",
       "      <th>Exponential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yotta</td>\n",
       "      <td>Y</td>\n",
       "      <td>1000000000000000000000000</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zetta</td>\n",
       "      <td>Z</td>\n",
       "      <td>1000000000000000000000</td>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exa</td>\n",
       "      <td>E</td>\n",
       "      <td>1000000000000000000</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peta</td>\n",
       "      <td>P</td>\n",
       "      <td>1000000000000000</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tera</td>\n",
       "      <td>T</td>\n",
       "      <td>1000000000000</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefix Symbol                 Multiplier Exponential\n",
       "1  yotta      Y  1000000000000000000000000        1024\n",
       "2  zetta      Z     1000000000000000000000        1021\n",
       "3    exa      E        1000000000000000000        1018\n",
       "4   peta      P           1000000000000000        1015\n",
       "5   tera      T              1000000000000        1012"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tables[0]\n",
    "df = df.rename(columns=df.iloc[0])\n",
    "df = df[df.index > 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "When web developers make heavy use of div tags or classes this often makes our life easier because we can target specific tags or attributes to get the information we desire, however sometimes this is not the case. We need to inspect the page with dev tools and try to spot some kind of pattern. In this case I wanted all of the relevant text from a wikipedia article, the first pattern I noticed was that all of the headings have a class of `mw-headline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mathematics',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Astronomy',\n",
       " 'Religion',\n",
       " 'Popular culture',\n",
       " \"The Hitchhiker's Guide to the Galaxy\",\n",
       " 'Works of Lewis Carroll',\n",
       " 'Music',\n",
       " 'Television and film',\n",
       " 'Video games',\n",
       " 'Sports',\n",
       " 'Gaming',\n",
       " 'Architecture',\n",
       " 'Other fields',\n",
       " 'Other languages',\n",
       " 'References',\n",
       " 'External links']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "headings = [ h.get_text() for h in soup.find_all(class_=\"mw-headline\")]\n",
    "headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use those headings as a key in a dictionary, allowing us to keep track of all of that sections text. As we mentioned ealier the HTML document is a type of tree, this means we can use recursive tree algorithms traverse it! We will traverse all of the elements and when we find a element with the class `mw-headline` we'll extract the bellow text and store it in our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Architecture': '',\n",
       " 'Astronomy': '',\n",
       " 'External links': '',\n",
       " 'Gaming': '',\n",
       " 'Mathematics': '',\n",
       " 'Music': '',\n",
       " 'Other fields': '',\n",
       " 'Other languages': '',\n",
       " 'Popular culture': '',\n",
       " 'References': '',\n",
       " 'Religion': '',\n",
       " 'Science': '',\n",
       " 'Sports': '',\n",
       " 'Technology': '',\n",
       " 'Television and film': '',\n",
       " \"The Hitchhiker's Guide to the Galaxy\": '',\n",
       " 'Video games': '',\n",
       " 'Works of Lewis Carroll': '',\n",
       " 'garbage': ''}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {heading:\"\" for heading in headings}\n",
    "k = \"garbage\" #add garbge key for stuff we don't care about\n",
    "d[k] = \"\"\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = [\"li\",\"a\",\"p\",\"span\"] #only care about text containing nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use recursive generator to loop over all of the nodes\n",
    "for element in soup.recursiveChildGenerator():\n",
    "    if element.name in tag_names:\n",
    "        if element.has_attr('class') and 'mw-headline' in element['class']:\n",
    "            k = element.text\n",
    "        d[k] += element.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecture[edit][edit]The architects of the Rockefeller Center in New York City worked daily in the Graybar Building where on \"the twenty-fifth floor, one enormous drafting room contained forty-two identical drawing boards, each the size of a six-seat dining room table; another room harboured twelve more, and an additional fourteen stood just outside the principals\\' offices at the top of the circular iron staircase connecting 25 to 26\".[26]Rockefeller CenterNew York CityGraybar Building[26]In the Rockefeller Center (New York City) there are a total of \"forty-two elevators in five separate banks\"[27] which carry tenants and visitors to the sixty-six floors.Rockefeller CenterNew York City[27]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Architecture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science[edit][edit]42 is the atomic number of molybdenum.atomic numbermolybdenum42 is the atomic mass of one of the naturally occurring stable isotopes of calcium.atomic masscalciumThe angle rounded to whole degrees for which a rainbow appears (the critical angle).rainbowIn 1966, mathematician Paul Cooper theorized that the fastest, most efficient way to travel across continents would be to bore a straight hollow tube directly through the Earth, connecting a set of antipodes, remove the air from the tube and fall through.[9] The first half of the journey consists of free-fall acceleration, while the second half consists of an exactly equal deceleration. The time for such a journey works out to be 42\\xa0minutes. Even if the tube does not pass through the exact center of the Earth, the time for a journey powered entirely by gravity (known as a gravity train) always works out to be 42\\xa0minutes, so long as the tube remains friction-free, as while the force of gravity would be lessened, the distance traveled is reduced at an equal rate.[10][11] (The same idea was proposed, without calculation by Lewis Carroll in 1893 in Sylvie and Bruno Concluded.[12]) Now we know that is not true, and it only would take about 38 minutes.Earthantipodes[9]gravity train[10][11]Lewis CarrollSylvie and Bruno Concluded[12]38 minutes.As determined by the Babylonians, in 79 years Mars orbits the Sun almost 42 times.[13][13]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step could be further clean the text, maybe we want to remove the `[edit]` from the text, this could be done with a regex. We may also wish to get the links for the references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {k: re.sub('\\[edit\\]',\"\",v) for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science42 is the atomic number of molybdenum.atomic numbermolybdenum42 is the atomic mass of one of the naturally occurring stable isotopes of calcium.atomic masscalciumThe angle rounded to whole degrees for which a rainbow appears (the critical angle).rainbowIn 1966, mathematician Paul Cooper theorized that the fastest, most efficient way to travel across continents would be to bore a straight hollow tube directly through the Earth, connecting a set of antipodes, remove the air from the tube and fall through.[9] The first half of the journey consists of free-fall acceleration, while the second half consists of an exactly equal deceleration. The time for such a journey works out to be 42\\xa0minutes. Even if the tube does not pass through the exact center of the Earth, the time for a journey powered entirely by gravity (known as a gravity train) always works out to be 42\\xa0minutes, so long as the tube remains friction-free, as while the force of gravity would be lessened, the distance traveled is reduced at an equal rate.[10][11] (The same idea was proposed, without calculation by Lewis Carroll in 1893 in Sylvie and Bruno Concluded.[12]) Now we know that is not true, and it only would take about 38 minutes.Earthantipodes[9]gravity train[10][11]Lewis CarrollSylvie and Bruno Concluded[12]38 minutes.As determined by the Babylonians, in 79 years Mars orbits the Sun almost 42 times.[13][13]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the reference links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'https://oeis.org/A002378',\n",
       " 2: 'https://oeis.org/A054377',\n",
       " 3: 'https://oeis.org/A000108',\n",
       " 4: 'https://oeis.org/A051867',\n",
       " 5: 'http://www.mathpages.com/home/kmath255.htm',\n",
       " 6: 'http://oeis.org/A019283',\n",
       " 7: 'https://web.archive.org/web/20070519144253/http://www.uni.uiuc.edu/gargoyle/2007/05/zhai_poised_to_represent_unite.htm',\n",
       " 8: 'https://archive.is/20120721120006/http://www.cbc.ca/health/story/2004/07/20/math_win040720.html',\n",
       " 9: 'http://adsabs.harvard.edu/abs/1966AmJPh..34...68C',\n",
       " 10: '//doi.org/10.1119%2F1.1972773',\n",
       " 11: 'http://www.time.com/time/magazine/article/0,9171,842469,00.html',\n",
       " 12: 'https://web.archive.org/web/20080512131156/http://www.time.com/time/magazine/article/0%2C9171%2C842469%2C00.html',\n",
       " 13: 'https://web.archive.org/web/20080602142755/https://www.youtube.com/watch?v=FAFUSbIs5KA',\n",
       " 14: 'https://www.youtube.com/watch?v=FAFUSbIs5KA',\n",
       " 15: 'http://www.eternalgadgetry.com/ancient_astronomy.html',\n",
       " 16: 'http://spiedl.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=JEIME5000011000001000104000001&idtype=cvips&gifs=yes&ref=no',\n",
       " 17: 'http://adsabs.harvard.edu/abs/2002JEI....11..104M',\n",
       " 18: '//doi.org/10.1117%2F1.1426078',\n",
       " 19: 'https://technet.microsoft.com/en-us/library/hh994573.aspx',\n",
       " 20: 'http://physics.ucsc.edu/cosmo/primack_abrams/InABeginningTikkun1995.pdf',\n",
       " 21: 'http://kasmana.people.cofc.edu/MATHFICT/mfview.php?callnumber=mf458',\n",
       " 22: 'https://www.theguardian.com/books/2011/feb/03/douglas-adams-42-hitchhiker?INTCMP=SRCH',\n",
       " 23: 'https://archive.is/20120629081121/http://findarticles.com/p/articles/mi_hb346/is_1_38/ai_n29162565/',\n",
       " 24: 'http://findarticles.com/p/articles/mi_hb346/is_1_38/ai_n29162565/',\n",
       " 25: 'http://www.snrk.de/snarkhunt/?newpics=no#preface',\n",
       " 26: 'http://www.slate.com/id/2245647/',\n",
       " 27: 'http://www.slate.com/id/2284721/',\n",
       " 28: 'https://www.lords.org/mcc/laws-of-cricket/',\n",
       " 29: 'http://futurezone.at/thema/start-ups/42-neues-ki-start-up-von-jajah-gruender-daniel-mattes/165.491.442',\n",
       " 30: 'http://scienceblogs.com/startswithabang/2009/08/31/paper-folding-to-the-moon/'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = soup.find(class_=\"references\")\n",
    "reference_links = [ a.get('href') for a in references.find_all(class_=\"external text\")]\n",
    "{ i+1 : link for i,link in enumerate(reference_links)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selectors\n",
    "\n",
    "Web developers use CSS selectors to select elements on the page they want to apply styling on. Hence CSS Selectors provide us with a succint way to specify which information on the html page we'd like to extract.  The simplest way to get to grips with them is to play one of these games:\n",
    "\n",
    "* [CSS Diner](https://flukeout.github.io/)\n",
    "* [CSS Leveler](http://toolness.github.io/css-selector-game/)\n",
    "\n",
    "Alternatively you could write a simple html page and try to style it.  For a good cheatsheet on CSS Selectors and Xpath see [here](http://www.cheetyr.com/css-selectors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "* ** 1. ** Play one of the CSS Selector Games.\n",
    "    * [CSS Diner](https://flukeout.github.io/)\n",
    "    * [CSS Leveler](http://toolness.github.io/css-selector-game/)\n",
    "* ** 2. ** Write a script that uses pandas `.read_html` to get all of the proxies from  https://free-proxy-list.net/ and write them to file.\n",
    "\n",
    "\n",
    "In the following exercises you may wish to use the proxies you gathered when you make request to the webpages, this can be done with:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "proxies = {\"http\": \"http://10.10.1.10:3128\",\n",
    "           \"https\": \"http://10.10.1.10:1080\"}\n",
    "\n",
    "requests.get(\"http://example.org\", proxies=proxies)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* ** 3. ** Extract all of the title names and links from [Hacker News](https://news.ycombinator.com/)\n",
    "* ** 4. ** Extract the `src` path for all of the images on [Reddit](https://www.reddit.com/)\n",
    "* ** 5. ** Bonus: Use beautiful soup to scrape a website of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "\n",
    "* [Beautiful Soup Sendex](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfV1MIRBOcqClP6VZXsvyZS)\n",
    "* [Beautiful soup docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [MDN Docs](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
