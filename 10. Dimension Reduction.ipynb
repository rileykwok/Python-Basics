{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction \n",
    "\n",
    "When we have a dataset with a large number of variables we need to ask questions like:\n",
    "\n",
    "* Are all of the variables important\n",
    "* How can I identify the signficant varaibles\n",
    "* Faced will all of these variables will my algorithm run to slow?\n",
    "\n",
    "Thankfully dimensionality reduction can help use here. Dimensionality reductions is the process of converting a set of data with many dimensions into data with less dimensions, it has two main use cases.\n",
    "\n",
    "* Helping to visualize higher dimensional features\n",
    "* Reducing the number of features in the dataset, so we can train model quicker/better.\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "The simplest form of dimensionality reduction would be feature selection, by only choosing to use say 3 features out of 10, we've reduced the dimensionality from 10D to 3D. Feature selection has a number of benefits inclduing:\n",
    "\n",
    "* Short training times\n",
    "* Avoiding the curse of dimensionality (covered further down)\n",
    "* Helping to reduce overfitting\n",
    "\n",
    "Imagine we had a variable with really low variance in the dataset ,say it's value is 3 for every observation. Does this variable add predictive power to the model? No because it's exactly same across all of the examples  What about if two variables are highly correlated. In such case feeding highly correlated varaibles to the model,  can confuse it since both contain similar information, it's almost like feeding the model duplicated information. We can use pearson correlation coeffient to check if the variables are correlated.  Plotting the correlation matrix as an heat map can be a easiy way to do this.  The above idea are almost like manual ways of performing dimensionality reduction however there are more stastical approaches such as factor analysis, explained bellow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Dimensionality reduction is useful for two reason:\n",
    "\n",
    "* It makes it easier to view higher dimensional data\n",
    "* Can reduce computational cost of running learning algorithms.\n",
    "\n",
    "PCA or principal compenent analysis, is a dimensionality reduction technqiue that allows us to more easily visulize higher dimensional data. This is achieved by applying a linear transform to the data, we'll cover more on linear transformation later. For now think of it as rotating it so we can view it from the best angle.  The way PCA finds the new axis for the rotation is by looking for the directions of maximum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "* Standadize the data\n",
    "* Calculate eigen values\n",
    "* Transform the original $ X $ data set using $ W $ to comain a new $ k $ dimensional feature subspace $ Y $\n",
    "* Sort the eigen values in descending order and choose $ k $ eigen vectors that correspond to the $ k $ largest eigen vecots\n",
    "* Construct the projection matrix W from the $ k $ eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "X = df.iloc[:,:4].values\n",
    "y =df.iloc[:,4].values\n",
    "X.shape #150 examples, each example has 4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\mathbf{x^T} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} \n",
    "= \\begin{pmatrix} \\text{sepal length} \\\\ \\text{sepal width} \\\\\\text{petal length} \\\\ \\text{petal width} \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "We need to standardize the data before we can calculate the eigen values, this can be achieved by subtracting the mean and dividng by standard deviation. This rescales the data to have a mean $ \\mu $ of 0 and a standard deviation $ \\sigma $ or 1 (unit of variance).\n",
    "\n",
    "$$ {\\frac {X-\\mu }{\\sigma }} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = X.mean(axis=0)\n",
    "sigma = X.std(axis=0).mean()\n",
    "X= (X - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvectors and Eigenvalues\n",
    "\n",
    "We can obtain the eigen vectors and values through various approaches,such as eigendecomposition of the covariance matrix or SVD (singular value decomposition) However in practice most implementations use SVD (singular value decomposition) since it's the least computationally expensive, so well use that. We'll get onto what the eigenvectors and values mean later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors, eigenvalues, _ = np.linalg.svd(X.T, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36138659 -0.65658877  0.58202985  0.31548719]\n",
      " [ 0.08452251 -0.73016143 -0.59791083 -0.3197231 ]\n",
      " [-0.85667061  0.17337266 -0.07623608 -0.47983899]\n",
      " [-0.3582892   0.07548102 -0.54583143  0.75365743]]\n"
     ]
    }
   ],
   "source": [
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.56917643  6.36512452  3.61349905  1.99483333]\n"
     ]
    }
   ],
   "source": [
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data\n",
    "\n",
    "Now that we've calculated the eigenvectors $ W $ we can use them to transform the data $X$ with a matrix multiplication.\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{X} \\times  \\mathbf{W} $$\n",
    "\n",
    "\n",
    "\n",
    "What does the eigen vectors and value represent? Eigen vectors show us the directions with the most variance and the eigen values tell us how much variance is explained in that direction. The eigen vectors allow us to transfrom the data so we can view it from an angle that captures the most variance in it. To illustrate this look at the plot bellow, the eigen values capture the direction of most variance. We can use the eigen values to rotate the data in a way that makes it easier to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = X @ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plot_pca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-759ea8e2887a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplot_pca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_pca_illustration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_pca_illustration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plot_pca'"
     ]
    }
   ],
   "source": [
    "from plot_pca import plot_pca_illustration\n",
    "plot_pca_illustration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to reduce the dimensionality of the data, we won't use all o the eigen values. After sorting we'll  need to pick which eigenvectors we will use to construct ou new subspace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvectors))]\n",
    "eigenpairs.sort(reverse=True)\n",
    "\n",
    "#Reorganize eigen vectors by eigen values\n",
    "n = len(eigenvectors)\n",
    "eigenvectors = np.vstack([eigenpairs[i][1] for i in range(n)])\n",
    "eigenvalues = np.vstack([eigenpairs[i][0] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36138659,  0.08452251, -0.85667061, -0.3582892 ],\n",
       "       [-0.65658877, -0.73016143,  0.17337266,  0.07548102],\n",
       "       [ 0.58202985, -0.59791083, -0.07623608, -0.54583143],\n",
       "       [ 0.31548719, -0.3197231 , -0.47983899,  0.75365743]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26.56917643],\n",
       "       [ 6.36512452],\n",
       "       [ 3.61349905],\n",
       "       [ 1.99483333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explained variance\n",
    "\n",
    "This is a way to tell how accurate our 2D plot is. If the explained variance is across the first 2 principal components then the 2D plot is a good representation of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0a915f10f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a915db7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 4\n",
    "names = [f'PC {i}' for i in range(1, n  + 1)]\n",
    "df = pd.DataFrame({'Principal component':names,'Explained Variance':eigenvalues.ravel()/eigenvalues.sum()})\n",
    "sns.barplot(x='Principal component',y='Explained Variance',data= df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altogether\n",
    "\n",
    "Let put all the piece together to get our working implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X,n_components=2):\n",
    "\n",
    "    #standadize\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0).mean()\n",
    "    X= (X - mu) / sigma\n",
    "\n",
    "    # calculate eigen vectors and values\n",
    "    eigenvectors, eigenvalues, _ = np.linalg.svd(X.T, full_matrices=False)\n",
    "\n",
    "    #Sort eigen vectors by size of eigen value\n",
    "    eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvectors))]\n",
    "    eigenpairs.sort(reverse=True)\n",
    "\n",
    "    #Reorganize eigen vectors by eigen values\n",
    "    n_components = len(eigenvectors)\n",
    "    eigenvectors = np.vstack([eigenpairs[i][1] for i in range(n)])\n",
    "    eigenvalues = np.vstack([eigenpairs[i][0] for i in range(n)])\n",
    "\n",
    "    k = 2\n",
    "    return X @ eigenvectors, eigenvectors, eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "Just for sanity lets check our implementation against sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transfromed,eigenvectors,eigenvalues = pca(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sk = PCA(2)\n",
    "pca_sk.fit(X)\n",
    "X_transformed_sk =  pca_sk.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36138659, -0.08452251,  0.85667061,  0.3582892 ],\n",
       "       [ 0.65658877,  0.73016143, -0.17337266, -0.07548102]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sk.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36138659,  0.08452251, -0.85667061, -0.3582892 ],\n",
       "       [-0.65658877, -0.73016143,  0.17337266,  0.07548102],\n",
       "       [ 0.58202985, -0.59791083, -0.07623608, -0.54583143],\n",
       "       [ 0.31548719, -0.3197231 , -0.47983899,  0.75365743]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rounding error means not totally equal\n",
    "(np.matrix.round(X_transformed,5) ==  np.matrix.round(X_transformed_sk,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [Eigenvectors and values visually explained](http://setosa.io/ev/eigenvectors-and-eigenvalues/)\n",
    "* [Understanding Eigenvector and Egienvalues Visually](https://alyssaq.github.io/2015/understanding-eigenvectors-and-eigenvalues-visually/)\n",
    "* [Plotly PCA](https://plot.ly/ipython-notebooks/principal-component-analysis/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
